{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Code for ensemble deep learning method based on optimization of skill scoresn and value-weighted skill scores\n",
    "## Sabrina Guastavino, guastavino@dima.unige.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas\n",
    "import pickle\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import time\n",
    "import os.path\n",
    "import numpy\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities_scores as utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "A collection of models we'll use to attempt to classify videos.\n",
    "\n",
    "\"\"\"\n",
    "import keras\n",
    "import numpy\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.layers import (Conv2D,MaxPooling2D)\n",
    "\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from keras.layers import Activation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of weighted categorical crossentropy\n",
    "def weightedCatCELoss():\n",
    "    def weightLoss_(y_true,y_pred):\n",
    "\n",
    "        classes = K.argmax(y_true)\n",
    "        classCount = K.sum(y_true,axis=0)\n",
    "\n",
    "        loss = K.categorical_crossentropy(y_true, y_pred, from_logits=False)\n",
    "        return loss / K.gather(classCount, classes)\n",
    "\n",
    "    return weightLoss_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the input shape variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the variables of input shape\n",
    "\n",
    "#number of images in time series\n",
    "seq_length = 10\n",
    "#size of images\n",
    "image_shape = (128,256)\n",
    "number_channels = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the deep neural network model:\n",
    "#### Long-term recurrent neural network (LRCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 10, 64, 128, 8)    608       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 64, 128, 8)    32        \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 64, 128, 8)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 10, 31, 63, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 10, 16, 32, 16)    1168      \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 10, 16, 32, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 10, 16, 32, 16)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 10, 7, 15, 16)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 10, 4, 8, 32)      4640      \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 10, 4, 8, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 10, 4, 8, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 10, 1, 3, 32)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 10, 96)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                29400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 36,142\n",
      "Trainable params: 36,030\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LRCN MODEL\n",
    "\n",
    "# the input shape is composed by (number of images in time series, image height, image width, number of channels)\n",
    "\n",
    "#in the case no multi channel data are used set number_of_channels equal to 1\n",
    "\n",
    "input_shape = (seq_length, image_shape[0], image_shape[1], number_channels)\n",
    "\n",
    "initialiser = 'glorot_uniform'\n",
    "\n",
    "reg_lambda  = 0.01\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# first block\n",
    "model.add(TimeDistributed(Conv2D(8, (5, 5), strides=(2, 2), padding='same', \n",
    "                                               kernel_initializer=initialiser) , #, \n",
    "                                  input_shape=input_shape)) \n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((4, 4), strides=(2, 2))))\n",
    "\n",
    "\n",
    "# second block\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3), strides=(2, 2), padding='same',\n",
    "                                               kernel_initializer=initialiser), \n",
    "                                  input_shape=input_shape)) \n",
    "\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((4, 4), strides=(2, 2))))\n",
    "  \n",
    "# third block\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), strides=(2, 2), padding='same', \n",
    "                                               kernel_initializer=initialiser), \n",
    "                                  input_shape=input_shape)) \n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D((4, 4), strides=(2, 2))))\n",
    "\n",
    "\n",
    "\n",
    "# LSTM \n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50, return_sequences=False, dropout=0.5)) \n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "loss = weightedCatCELoss()\n",
    "metrics=['accuracy']\n",
    "optimizer = Adam(lr=1e-3, decay=1e-6)\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer,\n",
    "                               metrics=metrics)\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DEFINE TRAINING\n",
    "\n",
    "def train(model,batch_size=72, nb_epoch=100, \n",
    "          file_X_train='', file_y_train='',\n",
    "         file_X_val='', file_y_val=''):\n",
    "    \n",
    "    # function which returns the trained deep neural network given in input\n",
    "    # - model = the deep neural network model\n",
    "    # - batch_size = the size of the batch size\n",
    "    # - nb_epoch = number of epochs\n",
    "    # - file_X_train = the file in which the training X matrix is saved\n",
    "    # - file_y_train = the file in which the training y label vector is saved\n",
    "    # - file_X_val = the file in which the validation X matrix is saved\n",
    "    # - file_y_val = the file in which the validation y label vector is saved\n",
    "    \n",
    "    # tarining and validation data have to satisfy the following conditions:\n",
    "    # X_train must be of size (n_train, seq_length, image_shape[0], image_shape[1], number_channels)\n",
    "    # X_val must be of size (n_val, seq_length, image_shape[0], image_shape[1], number_channels)\n",
    "    # y_train is a binary vector of size n_train \n",
    "    # y_val is a binary vector of size n_val\n",
    "\n",
    "    # save the models for each epoch in data/checkpoints\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        filepath=os.path.join('data', 'checkpoints',  'lrcn_train_3levels_weighted_cat_cross_'+str(nb_epoch)+'epoch'+ \n",
    "            '.{epoch:03d}-{val_loss:.3f}.hdf5'), \n",
    "        verbose=1,\n",
    "        save_best_only=False) #\n",
    "\n",
    "    # Helper: Save results.\n",
    "    timestamp = time.time()\n",
    "    csv_logger = CSVLogger(os.path.join('data', 'logs', 'lrcn-' + 'training-' + \\\n",
    "        str(timestamp) + '.log'))\n",
    "\n",
    "    \n",
    "    # Load training and validation data\n",
    "    X_train = numpy.load(file_X_train)\n",
    "    yy_train =  numpy.load(file_y_train)\n",
    "    y_train = to_categorical(yy_train) #<-- with categorical crossentropy\n",
    "    X_val = numpy.load(file_X_val)\n",
    "    yy_val =  numpy.load(file_y_val)\n",
    "    y_val = to_categorical(yy_val)  #<-- with categorical crossentropy\n",
    "    model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            verbose=1,\n",
    "            callbacks=[csv_logger, checkpointer], \n",
    "            epochs=nb_epoch)\n",
    "    \n",
    "    # save the history\n",
    "    history_file = open('lrcn_history_train_3levels_batch_'+str(batch_size)+'_epoch_'+str(nb_epoch)+\n",
    "                  '_weighted_cat_cross.pkl', \"wb\")\n",
    "    pickle.dump(model.history.history, history_file)\n",
    "    history_file.close()\n",
    "\n",
    "    \n",
    "    #plot the loss function on training and validation set over epochs\n",
    "    mpl.style.use('default')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    plot_train = ax.plot(np.arange(1,len(model.history.history['loss'])+1),model.history.history['loss'],label='train loss',color='red', linewidth=1.9)\n",
    "    plot_val = ax.plot(np.arange(1,len(model.history.history['val_loss'])+1),model.history.history['val_loss'],label='val loss',color='blue',linewidth=1.9)\n",
    "    ax.set_xlabel('Epoch', fontsize=14)\n",
    "    #ax.set_xlim(1,len(model_history['val_loss']))\n",
    "    ax.legend(loc=\"upper right\", fontsize=14)\n",
    "    plt.savefig(\"plot_loss_lrcn_train_3levels_batch_\"+str(batch_size)+\"_epoch_\"+str(nb_epoch)+\n",
    "            \"_weighted_cat_cross.png\")\n",
    "    \n",
    "   \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Train the deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN the deep neural network\n",
    "\n",
    "seq_length = 10\n",
    "image_shape = (128,256)\n",
    "batch_size = 72 \n",
    "nb_epoch = 2 \n",
    "\n",
    "\n",
    "date_start_train ='20180709140000'#'20180902000000'\n",
    "date_end_train ='20190718230000'\n",
    "\n",
    "date_start_val = '20190719170000'\n",
    "date_end_val='20191003000000'\n",
    "\n",
    "# Define the filenames of the training set: the training matrix X_train and the label vector y_train\n",
    "file_X_train = ''\n",
    "file_y_train = ''\n",
    "# Define the filenames of the validation set: the validation matrix X_val and the label vector y_val\n",
    "file_X_val = ''\n",
    "file_y_val = ''\n",
    "\n",
    "\n",
    "model_history=train(model,batch_size=batch_size, nb_epoch=nb_epoch, \n",
    "                    file_X_train=file_X_train, file_y_train=file_y_train,\n",
    "                    file_X_val=file_X_val, file_y_val=file_y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Computation of optimal classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each epoch compute the optimal classification threshold by optimizing a desired skill \n",
    "# score on the training set and compute the desired skill score on the validation set\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "folder = 'data/checkpoints/'\n",
    "\n",
    "date_start_train ='20180709140000'#'20180902000000'\n",
    "date_end_train ='20190718230000'\n",
    "\n",
    "date_start_val = '20190719170000'\n",
    "date_end_val='20191003000000'\n",
    "\n",
    "\n",
    "path_data='data/'\n",
    "\n",
    "\n",
    "#load tarining and validation data\n",
    "X_train = np.load(path_data+'train/X_three_levels_resize_from_'+date_start_train+'_to_'+date_end_train+'.npy')#s#X_total_resize_from_20180709140000_to_20190718230000.npy')#X_random_128_NO_128_YES_from_20180709140000_to_20190902070000.npy,X_random_64_NO_64_YES_from_20180709140000_to_20181029090000.npy\n",
    "X_train = X_train[0:7128,:,:,:,:]\n",
    "y_train = np.load(path_data+'train/Y_three_levels_resize_from_'+date_start_train+'_to_'+date_end_train+'.npy') #Y_random_128_NO_128_YES_from_20180709140000_to_20190902070000.npy,Y_random_64_NO_64_YES_from_20180709140000_to_20181029090000.npy\n",
    "y_train = y_train[0:7128]#y_train[0:7128]#\n",
    "\n",
    "X_val = np.load(path_data+'val/X_three_levels_resize_from_'+date_start_val+'_to_'+date_end_val+'.npy')#X_total_resize_from_20190719170000_to_20191003000000.npy')#X_random_49_NO_47_YES_from_20190902080000_to_20191122110000.npy,X_random_32_NO_32_YES_from_20181029100000_to_20190701150000.npy\n",
    "X_val = X_val[0:1296,:,:,:,:]\n",
    "y_val = np.load(path_data+'val/Y_three_levels_resize_from_'+date_start_val+'_to_'+date_end_val+'.npy')\n",
    "y_val = y_val[0:1296]#y_val[0:1296]#\n",
    "\n",
    "\n",
    "batch_size = 72 \n",
    "nb_epoch = 2 \n",
    "#------------------------------------------------------------------------------------------------------\n",
    "# Load training and validation data\n",
    "# Define the filenames of the training set: the training matrix X_train and the label vector y_train\n",
    "file_X_train = ''\n",
    "file_y_train = ''\n",
    "# Define the filenames of the validation set: the validation matrix X_val and the label vector y_val\n",
    "file_X_val = ''\n",
    "file_y_val = ''\n",
    "\n",
    "\n",
    "X_train = np.load(file_X_train)\n",
    "y_train = np.load(file_y_train) \n",
    "\n",
    "\n",
    "X_val = np.load(file_X_val)\n",
    "y_val = np.load(file_y_val)\n",
    "\n",
    "# list of epochs: they have to be saved in folder\n",
    "folder = 'data/checkpoints/'\n",
    "list_epochs = sorted(glob.glob(folder+'lrcn_train_3levels_weighted_cat_cross_'+str(nb_epoch)+'*.hdf5'))\n",
    "\n",
    "file_name = 'train_3levels_weighted_cat_cross_100epoch'\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "\n",
    "tss_train_dict_opt_tss=[]\n",
    "tss_val_dict_opt_tss=[]\n",
    "hss_train_dict_opt_tss=[]\n",
    "hss_val_dict_opt_tss=[]\n",
    "csi_train_dict_opt_tss=[]\n",
    "csi_val_dict_opt_tss=[]\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "threshold_opt_tss_weight=[]\n",
    "threshold_opt_tss=[]\n",
    "\n",
    "\n",
    "\n",
    "wtss_train_dict_opt_tss_weight=[]\n",
    "wtss_val_dict_opt_tss_weight=[]\n",
    "whss_train_dict_opt_tss_weight=[]\n",
    "whss_val_dict_opt_tss_weight=[]\n",
    "wcsi_train_dict_opt_tss_weight=[]\n",
    "wcsi_val_dict_opt_tss_weight=[]\n",
    "\n",
    "pred_train_all=np.zeros((len(list_epochs),len(y_train)))\n",
    "pred_val_all=np.zeros((len(list_epochs),len(y_val)))\n",
    "\n",
    "\n",
    "j=0\n",
    "\n",
    "for file in list_epochs:\n",
    "    print(file)\n",
    "    model = load_model(file,compile=False)\n",
    "    pred_train = model.predict(X_train)\n",
    "    gc.collect()\n",
    "    pred_val = model.predict(X_val)\n",
    "    gc.collect()\n",
    "\n",
    "    pred_prob=pred_train[:,1]#<-- if use cat ce\n",
    "    pred_prob_val = pred_val[:,1]#<-- if use cat ce\n",
    "    \n",
    "    pred_train_all[j,:]=pred_prob.reshape(1,len(pred_prob))[0]\n",
    "    pred_val_all[j,:]=pred_prob_val.reshape(1,len(pred_prob_val))[0]\n",
    "    \n",
    "    #computation of the best threshold by optimizing value-weighted skill scores (as wNSS, wTSS, wHSS, ..)\n",
    "    threshold_nss_weight, metrics_training_weight, nss_vector_weight, threshold_tss_weight, threshold_hss_weight, threshold_csi_weight, threshold_tss_hss_weight = utilities.optimize_threshold_skill_scores_weight_matrix(pred_prob, y_train)\n",
    "    \n",
    "    \n",
    "    threshold_opt_tss_weight.append(threshold_tss_weight)\n",
    "    \n",
    "    #computation of the best threshold by optimizing skill scores (as NSS, TSS, HSS, ..)\n",
    "    threshold_nss, metrics_training, nss_vector, threshold_tss, threshold_hss, threshold_csi, threshold_tss_hss = utilities.optimize_threshold_skill_scores(pred_prob, y_train)\n",
    "    \n",
    "    threshold_opt_tss.append(threshold_tss)\n",
    "\n",
    "    \n",
    "    j = j + 1\n",
    "\n",
    "    \n",
    "\n",
    "    #compute value-weighted skill scores with optimum threshold computed by optimizing the wTSS\n",
    "    #on training set\n",
    "    cm, wtss, whss, wcsi  = utilities.compute_weight_cm_tss_threshold(y_train, pred_prob,threshold_tss_weight)\n",
    "    wtss_train_dict_opt_tss_weight.append(wtss)\n",
    "    whss_train_dict_opt_tss_weight.append(whss)\n",
    "    wcsi_train_dict_opt_tss_weight.append(wcsi)\n",
    "    #on validation set\n",
    "    wcm_val, wtss_val, whss_val, wcsi_val = utilities.compute_weight_cm_tss_threshold(y_val, pred_prob_val,threshold_tss_weight)\n",
    "    wtss_val_dict_opt_tss_weight.append(wtss_val)\n",
    "    whss_val_dict_opt_tss_weight.append(whss_val)\n",
    "    wcsi_val_dict_opt_tss_weight.append(wcsi_val)\n",
    "    print ('threshold best wtss                 \\t', threshold_tss_weight)\n",
    "    \n",
    "    #compute skill scores with optimum threshold computed by optimizing the TSS\n",
    "    #on training set\n",
    "    cm, tss, hss, csi  = utilities.compute_cm_tss_threshold(y_train, pred_prob,threshold_tss)\n",
    "    tss_train_dict_opt_tss.append(tss)\n",
    "    hss_train_dict_opt_tss.append(hss)\n",
    "    csi_train_dict_opt_tss.append(csi)\n",
    "    #on validation set\n",
    "    cm_val, tss_val, hss_val, csi_val = utilities.compute_cm_tss_threshold(y_val, pred_prob_val,threshold_tss)\n",
    "    tss_val_dict_opt_tss.append(tss_val)\n",
    "    hss_val_dict_opt_tss.append(hss_val)\n",
    "    csi_val_dict_opt_tss.append(csi_val)\n",
    "    print ('threshold best tss                 \\t', threshold_tss)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Computation of the level which defines the goodness of predictions on the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for the computation of the level which defines the goodness of predictions on the validation set\n",
    "\n",
    "def choice_level(list_epochs,score_val_epochs,threshold_opt_score,X_val,y_val,pred_val_all,score='wtss'):\n",
    "    # function which returns the optimal level which defines the goodness of predictions on the validation set\n",
    "    # - X_test = the input matrix of size (number of samples, seq_length, image_shape[0], image_shape[1], number_channels))\n",
    "    # - list_epochs = the file names in which the weights of the trained deep neural network are saved for all epochs\n",
    "    # - score_val_epochs = vector which contains the score computed on the validation set along epochs\n",
    "    # - gamma_best = a value in (0,1] which represents the optimal level computed at the third step\n",
    "    # - threshold_opt = vector which contains the optimal classification thresholds along epochs    \n",
    "    \n",
    "    n_samples=20\n",
    "    alpha_vector = numpy.zeros(n_samples)\n",
    "    score_val_vector = []\n",
    "    idx_list = []\n",
    "    a = 0.99\n",
    "    b = 0.8\n",
    "    step = 1. / n_samples\n",
    "    for alpha_idx in range(0, n_samples):\n",
    "        alpha_level = step * alpha_idx * numpy.abs(a - b) + b\n",
    "        alpha_vector[alpha_idx] = alpha_level\n",
    "        pred_0_1_list=[]\n",
    "\n",
    "\n",
    "        tt = np.max(np.array(score_val_epochs))*alpha_level \n",
    "\n",
    "\n",
    "        idx_ensemble=np.where(np.array(score_val_epochs)>tt)\n",
    "        idx_ensemble=idx_ensemble[0]\n",
    "        idx_list.append(idx_ensemblet)\n",
    "\n",
    "\n",
    "        for i in idx_ensemble:#50\n",
    "            file=list_epochs[i]\n",
    "            print(file)\n",
    "          \n",
    "            pred_prob_val = pred_val_all[i,:]\n",
    "            gc.collect()\n",
    "\n",
    "            pred_0_1 = pred_prob_val > threshold_opt_score[i]\n",
    "            pred_0_1_list.append(pred_0_1)\n",
    "\n",
    "       \n",
    "        pred_0_1_arr=numpy.array(pred_0_1_list)*1\n",
    "\n",
    "        pred_median_pred_0_1=numpy.median(pred_0_1_arr,axis=0)\n",
    "\n",
    "        idx_to_discard=numpy.where(pred_median_pred_0_1==0.5)[0]\n",
    "\n",
    "        pred_median_pred_0_1[idx_to_discard]=1\n",
    "        \n",
    "        if score == 'wtss':\n",
    "            wcm_val, wtss_val, whss_val, wcsi_val = utilities.compute_weight_cm_tss(y_val, pred_median_pred_0_1)\n",
    "            print('alpha_level = ',alpha_level) \n",
    "            print('Weighted Skill scores (wtss optimization)')\n",
    "            print(wcm_val)\n",
    "            print('wtss = ','{:0.4f}'.format(wtss_val))\n",
    "            print('whss = ','{:0.4f}'.format(whss_val))\n",
    "            print('wcsi = ','{:0.4f}'.format(wcsi_val))\n",
    "            score_val_vector.append(wtss_val)\n",
    "            \n",
    "        if score == 'tss':\n",
    "            cm_val, tss_val, hss_val, csi_val = utilities.compute_cm_tss(y_val, pred_median_pred_0_1)#_weight\n",
    "            print('alpha_level = ',alpha_level) \n",
    "            print('Skill scores (tss optimization)')\n",
    "            print(cm_val)\n",
    "            print('tss = ','{:0.4f}'.format(tss_val))\n",
    "            print('hss = ','{:0.4f}'.format(hss_val))\n",
    "            print('csi = ','{:0.4f}'.format(csi_val))\n",
    "            score_val_vector.append(tss_val)\n",
    "        \n",
    "    score_val_vector = np.array(score_val_vector)\n",
    "    idx_alpha_best = np.where(score_val_vector == np.max(score_val_vector))\n",
    "    alpha_best = alpha_vector[idx_alpha_best]\n",
    "    idx_list = np.array(idx_list)\n",
    "    idx_best = idx_list[idx_alpha_best]\n",
    "    return alpha_best, score_val_vector, alpha_vector, idx_alpha_best, idx_best\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the level  which defines the goodness of predictions on the validation set (the goodness is measured \n",
    "# w.r.t. the desired skill score)\n",
    "\n",
    "# the level alpha in the case of the wtss optimization strategy \n",
    "gamma_best_weight, wtss_val_vector_thresholds, alpha_vector_weight,idx_gamma_best_weight,idx_weight_best=choice_level(list_epochs,\n",
    "                                                                                  wtss_val_dict_opt_tss_weight,\n",
    "                                                                                  threshold_opt_tss_weight,\n",
    "                                                                                  X_val,y_val,pred_val_all)\n",
    "print('gamma computed with the wtss optimization strategy: ', gamma_best_weight)\n",
    "\n",
    "\n",
    "\n",
    "# the level alpha in the case of the tss optimization strategy \n",
    "gamma_best, tss_val_vector_thresholds, alpha_vector,idx_gamma_best,idx_best=choice_level(list_epochs,\n",
    "                                                                                  tss_val_dict_opt_tss,\n",
    "                                                                                  threshold_opt_tss,\n",
    "                                                                                  X_val,y_val,pred_val_all)\n",
    "\n",
    "print('gamma computed with the tss optimization strategy: ', gamma_best)\n",
    "\n",
    "\n",
    "\n",
    "#save variables in data/results\n",
    "#np.save('data/results/wtss_val_vector_thresholds_'+file_name+'.npy',wtss_val_vector_thresholds)\n",
    "#np.save('data/results/gamma_wtss_'+file_name+'.npy',gamma_best_weight)\n",
    "#np.save('data/results/tss_val_vector_thresholds_'+file_name+'.npy',tss_val_vector_thresholds)\n",
    "#np.save('data/results/gamma_tss_'+file_name+'.npy',gamma_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which defines predictions\n",
    "\n",
    "def ensemble_prediction(X_test,list_epochs,score_val_epochs,gamma_best,threshold_opt):#wtss_val_dict_opt_tss_weig\n",
    "    # function which returns the binary prediction using the ensemble method given in input X_test\n",
    "    # - X_test = the input matrix of size (number of samples, seq_length, image_shape[0], image_shape[1], number_channels))\n",
    "    # - list_epochs = the file names in which the weights of the trained deep neural network are saved for all epochs\n",
    "    # - score_val_epochs = vector which contains the score computed on the validation set along epochs\n",
    "    # - gamma_best = a value in (0,1] which represents the optimal level computed at the third step\n",
    "    # - threshold_opt = vector which contains the optimal classification thresholds along epochs\n",
    "    \n",
    "    pred_0_1_list=[]\n",
    "\n",
    "\n",
    "    alpha = np.max(np.array(score_val_epochs))*gamma_best[0] \n",
    "\n",
    "\n",
    "    idx_epochs=np.where(np.array(score_val_epochs)>alpha)\n",
    "    idx_epochs=idx_epochs[0]\n",
    "    print('List of epochs involved in the ensemble prediction')\n",
    "    for i in idx_epochs:\n",
    "        file=list_epochs[i]\n",
    "        print(file)\n",
    "        model = load_model(file,compile=False)\n",
    "        pred_test = model.predict(X_test)\n",
    "        pred_prob_test =pred_test[:,1]#if use cat CE..\n",
    "        pred_prob_test = pred_prob_test.reshape(1,len(pred_prob_test))\n",
    "        pred_prob_test = pred_prob_test[0]\n",
    "        gc.collect()\n",
    "\n",
    "        pred_0_1 = pred_prob_test > threshold_opt[i]\n",
    "        pred_0_1_list.append(pred_0_1)\n",
    "\n",
    "    pred_0_1_arr=numpy.array(pred_0_1_list)*1\n",
    "\n",
    "    pred_test=numpy.median(pred_0_1_arr,axis=0)\n",
    "\n",
    "    idx_to_discard=numpy.where(pred_test==0.5)[0]\n",
    "    pred_test[idx_to_discard]=1\n",
    "    \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on a test set\n",
    "\n",
    "folder = 'data/checkpoints/'\n",
    "\n",
    "# define the file names of test data: the test matrix X_test and the test label vector y_test\n",
    "file_X_test = ''\n",
    "file_y_test = ''\n",
    "\n",
    "\n",
    "# The matrix X_test must be of size (number of test samples, seq_length, image_shape[0], image_shape[1], number_channels))\n",
    "X_test = np.load(file_X_test) \n",
    "y_test = np.load(file_y_test)\n",
    "\n",
    "print('**** wTSS optimization strategy ****')\n",
    "pred_test_wtss_strategy = ensemble_prediction(X_test,list_epochs,wtss_val_dict_opt_tss_weight,gamma_best_weight,threshold_opt_tss_weight)\n",
    "print('**** TSS optimization strategy ****')\n",
    "pred_test_tss_strategy = ensemble_prediction(X_test,list_epochs,tss_val_dict_opt_tss,gamma_best,threshold_opt_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results on the test set\n",
    "\n",
    "##################################wTSS optimization strategy############################################\n",
    "\n",
    "#compute scores on test set\n",
    "cm_test, tss_test, hss_test, csi_test = utilities.compute_cm_tss(y_test, pred_test_wtss_strategy)\n",
    "print('**** wTSS optimization strategy ****')\n",
    "print('Skill scores on the test set')\n",
    "print(cm_test)\n",
    "print('tss = ','{:0.4f}'.format(tss_test))\n",
    "print('hss = ','{:0.4f}'.format(hss_test))\n",
    "print('csi = ','{:0.4f}'.format(csi_test))\n",
    "\n",
    "#compute value-weighted skill scores on test set\n",
    "wcm_test, wtss_test, whss_test, wcsi_test = utilities.compute_weight_cm_tss(y_test, pred_test_wtss_strategy)\n",
    "print('Value-weighted Skill scores on the test set')\n",
    "print(wcm_test)\n",
    "print('wtss = ','{:0.4f}'.format(wtss_test))\n",
    "print('whss = ','{:0.4f}'.format(whss_test))\n",
    "print('wcsi = ','{:0.4f}'.format(wcsi_test))\n",
    "\n",
    "###############################TSS optimization strategy##############################################\n",
    "print('**** TSS optimization strategy ****')\n",
    "#compute scores on test set\n",
    "cm_test, tss_test, hss_test, csi_test = utilities.compute_cm_tss(y_test, pred_test_tss_strategy)\n",
    "print('Skill scores on the test set')\n",
    "print(cm_test)\n",
    "print('tss = ','{:0.4f}'.format(tss_test))\n",
    "print('hss = ','{:0.4f}'.format(hss_test))\n",
    "print('csi = ','{:0.4f}'.format(csi_test))\n",
    "\n",
    "#compute value-weighted skill scores on test set\n",
    "wcm_test, wtss_test, whss_test, wcsi_test = utilities.compute_weight_cm_tss(y_test, pred_test_tss_strategy)\n",
    "print('Value-weighted skill scores on the test set')\n",
    "print(wcm_test)\n",
    "print('wtss = ','{:0.4f}'.format(wtss_test))\n",
    "print('whss = ','{:0.4f}'.format(whss_test))\n",
    "print('wcsi = ','{:0.4f}'.format(wcsi_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
